# Visualizing-Self-Attention-KV-Cache-Efficiency-in-Transformers
Interactive Streamlit app that visualizes Transformer self-attention and benchmarks KV-cache efficiency using DistilGPT-2. It allows exploration of attention across layers and heads and compares token-by-token text generation with and without KV-cache to demonstrate inference speedups in modern language models.
